{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "917bb261",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "538293bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "import collections\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from ucimlrepo import fetch_ucirepo   \n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def order(attri,i,datax):\n",
    "    for j in range(len(datax[i])):\n",
    "        if(attri == datax[i][j]):\n",
    "            return j\n",
    "        \n",
    "def attri(x, i, datax):\n",
    "    return datax[i][x]\n",
    "def get_s(X, loca):\n",
    "    s = []\n",
    "    if(loca > X.shape[1]):\n",
    "        return s\n",
    "    else:\n",
    "        for i in range(X.shape[1]):\n",
    "            if(i>=loca):\n",
    "                s.append(X.iloc[:, i].value_counts().index)\n",
    "        for i in range(X.shape[1]-loca):\n",
    "            s[i] = np.sort(s[i]) \n",
    "        return s \n",
    "\n",
    "def caa(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    accuracy\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    contingency = np.zeros((np.max(y_true) + 1, np.max(y_pred) + 1))\n",
    "    for i in range(len(y_true)):\n",
    "        contingency[y_true[i], y_pred[i]] += 1\n",
    "    \n",
    "    row_ind, col_ind = linear_sum_assignment(-contingency)\n",
    "    \n",
    "    mapping = {col: row for col, row in zip(col_ind, row_ind)}\n",
    "    y_pred_mapped = np.array([mapping[cluster] for cluster in y_pred])\n",
    "\n",
    "    return accuracy_score(y_true, y_pred_mapped)\n",
    "    \n",
    "def nmi(y,cluster_labels,real): \n",
    "    y = y.squeeze() \n",
    "    for i in range(len(real)):\n",
    "        for j in range(len(real)):\n",
    "            if(cluster_labels[i] == j):\n",
    "                cluster_labels[i] = real[j]\n",
    "\n",
    "    mi = mutual_info_score(y, cluster_labels)\n",
    "\n",
    "    h_y = entropy(np.unique(y, return_counts=True)[1] / len(y))\n",
    "    h_y_pred = entropy(np.unique(cluster_labels, return_counts=True)[1] / len(cluster_labels))\n",
    "\n",
    "    nmi = mi / max(h_y, h_y_pred)\n",
    "\n",
    "    print(f\"NMI: {nmi:.3f}\")\n",
    "    return nmi\n",
    "\n",
    "\n",
    "def nm_vo(y, cluster_labels):\n",
    "    y = y.squeeze() \n",
    "    vi1s = mutual_info_score(y, cluster_labels)\n",
    "    h_y = entropy(np.unique(y, return_counts=True)[1] / len(y))\n",
    "    h_y_pred = entropy(np.unique(cluster_labels, return_counts=True)[1] / len(cluster_labels))\n",
    "    return vi1s / max(h_y, h_y_pred) \n",
    "\n",
    "def ARI(cluster_labels,x):\n",
    "    x = x.squeeze()\n",
    "    ari = adjusted_rand_score(cluster_labels, x)\n",
    "    print(f\"Adjusted Rand Index (ARI): {ari:.3f}\")\n",
    "    return ari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfb6997",
   "metadata": {},
   "source": [
    "# CADM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e78152ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mutual_infoa(cen, q, attr_idx, intra_dis, opt):\n",
    "    gama_cen = intra_dis[attr_idx][cen]\n",
    "    if opt:\n",
    "        gama_q = intra_dis[attr_idx][q]\n",
    "    else:\n",
    "        gama_q = 1/intra_dis[attr_idx][q]\n",
    "        \n",
    "    mi_pq = gama_cen + gama_q\n",
    "    \n",
    "    return mi_pq\n",
    "\n",
    "def pairwise_distance_ca2(X, Cen, k1, data, location, datax, importance_freq, intra_dis, opt, attribute_ranks=None):\n",
    "    \"\"\"\n",
    "    CVD\n",
    "    :param X: sample\n",
    "    :param Cen: centroid\n",
    "    :return: the distance between two objects\n",
    "    \"\"\"\n",
    "    n = X.shape[1]\n",
    "    distance = 0\n",
    "    \n",
    "    p_array = X[0]\n",
    "    qc_array = Cen[0]\n",
    "    \n",
    "    for i in range(n):\n",
    "        p = p_array[i]\n",
    "        qc = qc_array[i]\n",
    "        \n",
    "        if i >= location:\n",
    "            if p == qc:\n",
    "                continue  \n",
    "            else:\n",
    "                \n",
    "                if attribute_ranks and i in attribute_ranks and p in attribute_ranks[i] and qc in attribute_ranks[i]:\n",
    "                    rank1 = attribute_ranks[i][p]\n",
    "                    rankc = attribute_ranks[i][qc]\n",
    "                else:\n",
    "                    rank1 = order(p, i-location, datax)\n",
    "                    rankc = order(qc, i-location, datax)\n",
    "                \n",
    "                diff = abs(rank1-rankc)\n",
    "                first = min(rank1, rankc)\n",
    "                gama_cen = intra_dis[i][qc]\n",
    "                \n",
    "                mutual_info_sum = 0\n",
    "                for j in range(0, diff+1):\n",
    "                    loc1 = first + j\n",
    "                    it = attri(loc1, i-location, datax)\n",
    "                    \n",
    "                    if it != qc:\n",
    "                        mutual_info = weighted_mutual_infoa(qc, it, i, intra_dis, opt) # CVD\n",
    "                        mutual_info_sum += mutual_info\n",
    "                \n",
    "                distance += mutual_info_sum\n",
    "                distance += importance_freq[i]*importance_freq[i] #CAI\n",
    "        else:\n",
    "            if p == qc:\n",
    "                continue \n",
    "            else:\n",
    "                mutual_info = weighted_mutual_infoa(qc, p, i, intra_dis, opt)\n",
    "                distance += mutual_info + importance_freq[i]*importance_freq[i]\n",
    "            \n",
    "    return distance\n",
    "\n",
    "def cluster_data_ca2(rs, df, location, datax, k1):   \n",
    "\n",
    "    df_numpy = df.values\n",
    "    \n",
    "    attribute_ranks = {}\n",
    "    for i in range(location, df.shape[1]):\n",
    "        idx = i - location\n",
    "        if idx < len(datax):\n",
    "            attribute_ranks[i] = {datax[idx][j]: j for j in range(len(datax[idx]))}\n",
    "    \n",
    "    k = k1\n",
    "\n",
    "    random_indices = np.random.RandomState(rs).choice(df.shape[0], k, replace=False)\n",
    "    centroids = df_numpy[random_indices]\n",
    "    \n",
    "    importance_freq = []\n",
    "    intra_dis = []\n",
    "    \n",
    "    for i1 in range(k):\n",
    "        importance_freq.append({})\n",
    "        intra_dis.append([])\n",
    "        for j in range(df.shape[1]):\n",
    "            intra_dis[i1].append({})\n",
    "                       \n",
    "    labels = np.zeros(df.shape[0], dtype=int)\n",
    "    cns = df.shape[0]\n",
    "    \n",
    "    min_distance = np.full(df.shape[0], np.inf)\n",
    "    \n",
    "    columns = df.columns\n",
    "    \n",
    "    for turn in range(100):\n",
    "        if turn > 10: \n",
    "            prev_labels = labels.copy()\n",
    "        \n",
    "        stable = True\n",
    "        opt = (turn == 0)\n",
    "        \n",
    "        for j in range(k):\n",
    "            if turn > 0:\n",
    "                cluster_mask = (labels == j)\n",
    "                cluster_points_df = df[cluster_mask]\n",
    "            else:\n",
    "                cluster_points_df = df\n",
    "            \n",
    "            for i in range(cluster_points_df.shape[1]):\n",
    "                value_counts = cluster_points_df.iloc[:, i].value_counts()\n",
    "                for val, count in value_counts.items():\n",
    "                    intra_dis[j][i][val] = count/cns\n",
    "            \n",
    "            s_t = 0\n",
    "            column_modes = []\n",
    "            for i1 in range(cluster_points_df.shape[1]):\n",
    "                counter = collections.Counter(cluster_points_df.iloc[:, i1])\n",
    "                most_common = counter.most_common(1)[0]\n",
    "                column_modes.append(most_common)\n",
    "                s_t += most_common[1]\n",
    "            \n",
    "            for i1 in range(cluster_points_df.shape[1]):\n",
    "                importance_freq[j][i1] = column_modes[i1][1]/s_t\n",
    "        \n",
    "        if df.shape[0] > 1000:\n",
    "            batch_size = min(100, df.shape[0] // 10)  \n",
    "            for batch_start in range(0, df.shape[0], batch_size):\n",
    "                batch_end = min(batch_start + batch_size, df.shape[0])\n",
    "                batch_indices = list(range(batch_start, batch_end))\n",
    "                \n",
    "                distances = []\n",
    "                for i in batch_indices:\n",
    "                    for j in range(k):\n",
    "                        dist = pairwise_distance_ca2(\n",
    "                            df_numpy[i:i+1], \n",
    "                            centroids[j:j+1], \n",
    "                            k1, df, location, \n",
    "                            datax, \n",
    "                            importance_freq[j], \n",
    "                            intra_dis[j], \n",
    "                            opt,\n",
    "                            attribute_ranks\n",
    "                        )\n",
    "                        distances.append(dist)\n",
    "                \n",
    "                distances = np.array(distances).reshape(len(batch_indices), k)\n",
    "                \n",
    "                new_labels = np.argmin(distances, axis=1)\n",
    "                new_min_distances = np.min(distances, axis=1)\n",
    "                \n",
    "                for idx, i in enumerate(batch_indices):\n",
    "                    if new_min_distances[idx] < min_distance[i]:\n",
    "                        min_distance[i] = new_min_distances[idx]\n",
    "                        if labels[i] != new_labels[idx]:\n",
    "                            labels[i] = new_labels[idx]\n",
    "                            stable = False\n",
    "        else:\n",
    "            for i in range(df.shape[0]):\n",
    "                for j in range(k):\n",
    "                    distance = pairwise_distance_ca2(\n",
    "                        df_numpy[i:i+1], \n",
    "                        centroids[j:j+1], \n",
    "                        k1, df, location, \n",
    "                        datax, \n",
    "                        importance_freq[j], \n",
    "                        intra_dis[j], \n",
    "                        opt,\n",
    "                        attribute_ranks\n",
    "                    )\n",
    "                    if distance < min_distance[i]:\n",
    "                        min_distance[i] = distance\n",
    "                        if labels[i] != j:\n",
    "                            labels[i] = j\n",
    "                            stable = False\n",
    "        \n",
    "        for j in range(k):\n",
    "            cluster_mask = (labels == j)\n",
    "            if np.any(cluster_mask):\n",
    "                cluster_points = df[cluster_mask]\n",
    "                new_centroids = []\n",
    "                \n",
    "                for col in columns:\n",
    "                    counter = collections.Counter(cluster_points[col])\n",
    "                    mode = counter.most_common(1)[0][0]\n",
    "                    new_centroids.append(mode)\n",
    "                \n",
    "                centroids[j] = np.array(new_centroids)\n",
    "        \n",
    "        if turn > 10:\n",
    "            change_ratio = np.sum(labels != prev_labels) / df.shape[0]\n",
    "            if change_ratio < 0.0001: \n",
    "                break\n",
    "                \n",
    "        if stable:\n",
    "            break\n",
    "\n",
    "    return labels, centroids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5bdeab",
   "metadata": {},
   "source": [
    "# CADM-m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2290e602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distance_cah2(X, Cen, k1, data, intloc, location, datax, importance_freq, intra_dis, opt, attribute_ranks=None):\n",
    "    #CVD\n",
    "    \n",
    "    n = X.shape[1]\n",
    "    distance = 0\n",
    "    \n",
    "    p_array = X[0]\n",
    "    qc_array = Cen[0]\n",
    "    \n",
    "    for i in range(n):\n",
    "        p = p_array[i]\n",
    "        qc = qc_array[i]\n",
    "        \n",
    "        if i < intloc:\n",
    "            if p == qc:\n",
    "                continue \n",
    "            else:\n",
    "                distance = abs(int(p)-int(qc)) + 1\n",
    "                \n",
    "        \n",
    "        if i >= location:\n",
    "            if p == qc:\n",
    "                continue \n",
    "            else:\n",
    "                if attribute_ranks and i in attribute_ranks and p in attribute_ranks[i] and qc in attribute_ranks[i]:\n",
    "                    rank1 = attribute_ranks[i][p]\n",
    "                    rankc = attribute_ranks[i][qc]\n",
    "                else:\n",
    "                    rank1 = order(p, i-location, datax)\n",
    "                    rankc = order(qc, i-location, datax)\n",
    "                \n",
    "                diff = abs(rank1-rankc)\n",
    "                first = min(rank1, rankc)\n",
    "                gama_cen = intra_dis[i][qc]\n",
    "                \n",
    "                mutual_info_sum = 0\n",
    "                for j in range(0, diff+1):\n",
    "                    loc1 = first + j\n",
    "                    it = attri(loc1, i-location, datax)\n",
    "                    \n",
    "                    if it != qc:\n",
    "                        mutual_info = weighted_mutual_infoa(qc, it, i, intra_dis, opt)\n",
    "                        mutual_info_sum += mutual_info\n",
    "                \n",
    "                distance += mutual_info_sum\n",
    "                distance += importance_freq[i]*importance_freq[i]\n",
    "                \n",
    "        else:\n",
    "            if p == qc:\n",
    "                continue  \n",
    "            \n",
    "    return distance\n",
    "\n",
    "def cluster_data_cah2(rs, df, intloc, location, datax, k1):   \n",
    "    \n",
    "    df_numpy = df.values\n",
    "\n",
    "    attribute_ranks = {}\n",
    "    for i in range(location, df.shape[1]):\n",
    "        idx = i - location\n",
    "        if idx < len(datax):\n",
    "            attribute_ranks[i] = {datax[idx][j]: j for j in range(len(datax[idx]))}\n",
    "    \n",
    "    k = k1\n",
    "    random_indices = np.random.RandomState(rs).choice(df.shape[0], k, replace=False)\n",
    "    centroids = df_numpy[random_indices]\n",
    "    \n",
    "    importance_freq = []\n",
    "    intra_dis = []\n",
    "    \n",
    "    for i1 in range(k):\n",
    "        importance_freq.append({})\n",
    "        intra_dis.append([])\n",
    "        for j in range(df.shape[1]):\n",
    "            intra_dis[i1].append({})\n",
    "                       \n",
    "    labels = np.zeros(df.shape[0], dtype=int)\n",
    "    cns = df.shape[0]\n",
    "    \n",
    "    min_distance = np.full(df.shape[0], np.inf)\n",
    "    \n",
    "    columns = df.columns\n",
    "    \n",
    "    for turn in range(100):\n",
    "        if turn > 10: \n",
    "            prev_labels = labels.copy()\n",
    "        \n",
    "        stable = True\n",
    "        opt = (turn == 0)\n",
    "\n",
    "        for j in range(k):\n",
    "            if turn > 0:\n",
    "                cluster_mask = (labels == j)\n",
    "                cluster_points_df = df[cluster_mask]\n",
    "            else:\n",
    "                cluster_points_df = df\n",
    "            \n",
    "            for i in range(cluster_points_df.shape[1]):\n",
    "                value_counts = cluster_points_df.iloc[:, i].value_counts()\n",
    "                for val, count in value_counts.items():\n",
    "                    intra_dis[j][i][val] = count/cns\n",
    "                    \n",
    "            s_t = 0\n",
    "            column_modes = []\n",
    "            for i1 in range(cluster_points_df.shape[1]):\n",
    "                counter = collections.Counter(cluster_points_df.iloc[:, i1])\n",
    "                most_common = counter.most_common(1)[0]\n",
    "                column_modes.append(most_common)\n",
    "                s_t += most_common[1]\n",
    "            \n",
    "            for i1 in range(cluster_points_df.shape[1]):\n",
    "                importance_freq[j][i1] = column_modes[i1][1]/s_t\n",
    "\n",
    "        if df.shape[0] > 1000:\n",
    "            batch_size = min(100, df.shape[0] // 10)  \n",
    "            for batch_start in range(0, df.shape[0], batch_size):\n",
    "                batch_end = min(batch_start + batch_size, df.shape[0])\n",
    "                batch_indices = list(range(batch_start, batch_end))\n",
    "                distances = []\n",
    "                for i in batch_indices:\n",
    "                    for j in range(k):\n",
    "                        dist = pairwise_distance_cah2(\n",
    "                            df_numpy[i:i+1], \n",
    "                            centroids[j:j+1], \n",
    "                            k1, df,intloc, location, \n",
    "                            datax, \n",
    "                            importance_freq[j], \n",
    "                            intra_dis[j], \n",
    "                            opt,\n",
    "                            attribute_ranks\n",
    "                        )\n",
    "                        distances.append(dist)\n",
    "                \n",
    "                distances = np.array(distances).reshape(len(batch_indices), k)\n",
    "                \n",
    "                new_labels = np.argmin(distances, axis=1)\n",
    "                new_min_distances = np.min(distances, axis=1)\n",
    "            \n",
    "                for idx, i in enumerate(batch_indices):\n",
    "                    if new_min_distances[idx] < min_distance[i]:\n",
    "                        min_distance[i] = new_min_distances[idx]\n",
    "                        if labels[i] != new_labels[idx]:\n",
    "                            labels[i] = new_labels[idx]\n",
    "                            stable = False\n",
    "        else:\n",
    "            for i in range(df.shape[0]):\n",
    "                for j in range(k):\n",
    "                    distance = pairwise_distance_cah2(\n",
    "                        df_numpy[i:i+1], \n",
    "                        centroids[j:j+1], \n",
    "                        k1, df, intloc, location, \n",
    "                        datax, \n",
    "                        importance_freq[j], \n",
    "                        intra_dis[j], \n",
    "                        opt,\n",
    "                        attribute_ranks\n",
    "                    )\n",
    "                    if distance < min_distance[i]:\n",
    "                        min_distance[i] = distance\n",
    "                        if labels[i] != j:\n",
    "                            labels[i] = j\n",
    "                            stable = False\n",
    "        \n",
    "        for j in range(k):\n",
    "            cluster_mask = (labels == j)\n",
    "            if np.any(cluster_mask):\n",
    "                cluster_points = df[cluster_mask]\n",
    "                new_centroids = []\n",
    "            \n",
    "                for col in columns:\n",
    "                    counter = collections.Counter(cluster_points[col])\n",
    "                    mode = counter.most_common(1)[0][0]\n",
    "                    new_centroids.append(mode)\n",
    "                \n",
    "                centroids[j] = np.array(new_centroids)\n",
    "\n",
    "        if turn > 10:\n",
    "            change_ratio = np.sum(labels != prev_labels) / df.shape[0]\n",
    "            if change_ratio < 0.01: \n",
    "                break\n",
    "                \n",
    "        if stable:\n",
    "            break\n",
    "\n",
    "    return labels, centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174cd025",
   "metadata": {},
   "source": [
    "# HA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "80eddcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset \n",
    "\n",
    "hayes_roth = fetch_ucirepo(id=44) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "\n",
    "X = hayes_roth.data.features \n",
    "y = hayes_roth.data.targets \n",
    "# delete null\n",
    "X = X.loc[~y.isnull().any(axis=1)]\n",
    "y = y.loc[~y.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "73044336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Index (ARI): 0.085\n",
      "NMI: 0.098\n",
      "Clustering Accuracy: 0.530\n"
     ]
    }
   ],
   "source": [
    "od = 2\n",
    "for i in range(1):\n",
    "    cluster_labels3, centroids = cluster_data_ca2(44,X,od,get_s(X, od),3)\n",
    "    haca3, real = cal(X,y,cluster_labels3)\n",
    "    haari3 =ARI(cluster_labels3,y)\n",
    "    hanmi3 = nmi(y,cluster_labels3,real)\n",
    "print(f\"Clustering Accuracy: {haca3:.3f}\")\n",
    "# print(f\"Adjusted Rand Index (ARI): {haari3:.3f}\")\n",
    "# print(f\"(NMI): {hanmi3:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f9ff09",
   "metadata": {},
   "source": [
    "# LS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5e47719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset \n",
    "lenses = fetch_ucirepo(id=58) \n",
    "  \n",
    "# # data (as pandas dataframes) \n",
    "X = lenses.data.features \n",
    "y = lenses.data.targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "49ad05f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Index (ARI): 0.481\n",
      "NMI: 0.327\n",
      "Clustering Accuracy: 0.750\n"
     ]
    }
   ],
   "source": [
    "od = 4\n",
    "for i in range(1):\n",
    "    cluster_labels3, centroids = cluster_data_ca2(999,X,od,get_s(X, od),3)\n",
    "    lsca3, real = cal(X,y,cluster_labels3)\n",
    "    lsari3 =ARI(cluster_labels3,y)\n",
    "    lsnmi3 = nmi(y,cluster_labels3,real)\n",
    "\n",
    "print(f\"Clustering Accuracy: {lsca3:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29145e89",
   "metadata": {},
   "source": [
    "# VT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c0f69c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset \n",
    "congressional_voting_records = fetch_ucirepo(id=105) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = congressional_voting_records.data.features \n",
    "y = congressional_voting_records.data.targets \n",
    "X = X.dropna()\n",
    "y = y.loc[X.index]\n",
    "y = y.replace({'democrat': 0, 'republican': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e0a673f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Index (ARI): 0.655\n",
      "NMI: 0.573\n",
      "Clustering Accuracy: 0.905\n"
     ]
    }
   ],
   "source": [
    "od = 17\n",
    "for i in range(1):\n",
    "    cluster_labels3, centroids = cluster_data_ca2(72,X,od,get_s(X, od),2)\n",
    "    vtca3, real = cal(X,y,cluster_labels3)\n",
    "    vtari3 =ARI(cluster_labels3,y)\n",
    "    vtnmi3 = nmi(y,cluster_labels3,real)\n",
    "\n",
    "print(f\"Clustering Accuracy: {vtca3:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f38dc57",
   "metadata": {},
   "source": [
    "# SM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "679dd524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "secondary_mushroom = fetch_ucirepo(id=848) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = secondary_mushroom.data.features \n",
    "y = secondary_mushroom.data.targets \n",
    "  \n",
    "X_cleaned = X.dropna(axis=1)\n",
    "X_cleaned = X_cleaned.drop(X_cleaned.columns[[0, 5, 6]], axis=1)\n",
    "y_replaced = y.replace({'p': 1, 'e': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3cc919bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 10.3323 seconds\n",
      "Clustering Accuracy: 0.553\n",
      "Adjusted Rand Index (ARI): 0.011\n",
      "(NMI): 0.007\n"
     ]
    }
   ],
   "source": [
    "od = 0\n",
    "for i in range(1):\n",
    "    start_time = time.time()\n",
    "    cluster_labels3, centroids3 = cluster_data_ca2(999,X_cleaned,od,get_s(X_cleaned, od),2)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f'Execution Time: {execution_time:.4f} seconds')\n",
    "    y1 = y_replaced\n",
    "    y2 = y_replaced.squeeze()  #\n",
    "    y3 = y_replaced.squeeze()  #\n",
    "    smca3 = caa(y1, cluster_labels3)\n",
    "    smari3 =adjusted_rand_score(y2,cluster_labels3)\n",
    "    smnmi3 =  normalized_mutual_info_score(y3, cluster_labels3)\n",
    "print(f\"Clustering Accuracy: {smca3:.3f}\")\n",
    "print(f\"Adjusted Rand Index (ARI): {smari3:.3f}\")\n",
    "print(f\"(NMI): {smnmi3:.3f}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ddc549",
   "metadata": {},
   "source": [
    "# NS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b1b33404",
   "metadata": {},
   "outputs": [],
   "source": [
    "nursery = fetch_ucirepo(id=76) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = nursery.data.features \n",
    "y = nursery.data.targets \n",
    "datax = [['usual', 'pretentious', 'great_pret'],\n",
    " ['proper', 'less_proper', 'improper', 'critical', 'very_crit'],\n",
    " ['complete', 'completed', 'incomplete', 'foster'],\n",
    " ['1', '2', '3', 'more'],\n",
    " ['convenient', 'less_conv', 'critical'],\n",
    " ['inconv', 'convenient'],\n",
    " ['nonprob', 'slightly_prob', 'problematic'],\n",
    " ['not_recom', 'recommended', 'priority']] # order information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e81b79a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 3.8123 seconds\n",
      "Clustering Accuracy: 0.441\n",
      "Adjusted Rand Index (ARI): 0.108\n",
      "(NMI): 0.103\n"
     ]
    }
   ],
   "source": [
    "od = 0\n",
    "start_time = time.time()\n",
    "cluster_labels3, centroids3 = cluster_data_ca2(999, X, od, datax, 4)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f'Execution Time: {execution_time:.4f} seconds')\n",
    "\n",
    "y2 = y.squeeze()  #\n",
    "y3 = y.squeeze()  #\n",
    "nsca3, real = cal(X, y, cluster_labels3)\n",
    "print(f\"Clustering Accuracy: {nsca3:.3f}\")\n",
    "nsari3 = adjusted_rand_score(y2, cluster_labels3)\n",
    "print(f\"Adjusted Rand Index (ARI): {nsari3:.3f}\")\n",
    "nsnmi3 = normalized_mutual_info_score(y3,cluster_labels3)\n",
    "print(f\"(NMI): {nsnmi3:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbf9c05",
   "metadata": {},
   "source": [
    "# HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "06642524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "heart_failure_clinical_records = fetch_ucirepo(id=519) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = heart_failure_clinical_records.data.features \n",
    "y = heart_failure_clinical_records.data.targets \n",
    "columns_to_move = [2, 4, 6,7, 8, 11]  \n",
    "\n",
    "remaining_columns = [i for i in range(X.shape[1]) if i not in columns_to_move]\n",
    "\n",
    "new_column_order = [X.columns[i] for i in columns_to_move] + [X.columns[i] for i in remaining_columns]\n",
    "\n",
    "X_reordered = X[new_column_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cd1a113b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering Accuracy: 0.742\n",
      "Adjusted Rand Index (ARI): 0.157\n",
      "(NMI): 0.145\n"
     ]
    }
   ],
   "source": [
    "intloc = 6\n",
    "od = 6\n",
    "for i in range(1):\n",
    "    cluster_labels, centroids = cluster_data_cah2(99, X_reordered, 6, 6,get_s(X_reordered, od),2) #44\n",
    "    y2 = y.squeeze()  #\n",
    "    y3 = y.squeeze()  #\n",
    "    \n",
    "    ca = caa(y, cluster_labels)\n",
    "    print(f\"Clustering Accuracy: {ca:.3f}\")\n",
    "    ari = adjusted_rand_score(y2, cluster_labels)\n",
    "    print(f\"Adjusted Rand Index (ARI): {ari:.3f}\")\n",
    "    hi1 = normalized_mutual_info_score(y3,cluster_labels)\n",
    "    print(f\"(NMI): {hi1:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
